{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eccce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import sentencepiece as spm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2934e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import pickle\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba509665",
   "metadata": {},
   "source": [
    "### Train, Test, Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7eeb4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from cleaned-text.txt...\n",
      "Total sentences: 204627\n",
      "\n",
      "Writing splits...\n",
      "  Train: 163701 sentences -> train.txt\n",
      "  Validation: 0 sentences -> val.txt\n",
      "  Test: 40926 sentences -> test.txt\n",
      "\n",
      "Split complete!\n",
      "  Train: 80.0%\n",
      "  Val: 0.0%\n",
      "  Test: 20.0%\n"
     ]
    }
   ],
   "source": [
    "def split_data(input_file, train_file, val_file, test_file, \n",
    "               train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Split data into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input text file (one sentence per line)\n",
    "        train_file: Output path for training data\n",
    "        val_file: Output path for validation data\n",
    "        test_file: Output path for test data\n",
    "        train_ratio: Fraction for training (default 0.8 = 80%)\n",
    "        val_ratio: Fraction for validation (default 0.1 = 10%)\n",
    "        test_ratio: Fraction for test (default 0.1 = 10%)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    # Check ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.001, \\\n",
    "        \"Ratios must sum to 1.0\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Read all lines\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    print(f\"Total sentences: {total_lines}\")\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_end = int(total_lines * train_ratio)\n",
    "    val_end = train_end + int(total_lines * val_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = lines[:train_end]\n",
    "    val_data = lines[train_end:val_end]\n",
    "    test_data = lines[val_end:]\n",
    "    \n",
    "    # Write to files\n",
    "    print(f\"\\nWriting splits...\")\n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    print(f\"  Train: {len(train_data)} sentences -> {train_file}\")\n",
    "    \n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(val_data))\n",
    "    print(f\"  Validation: {len(val_data)} sentences -> {val_file}\")\n",
    "    \n",
    "    with open(test_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    print(f\"  Test: {len(test_data)} sentences -> {test_file}\")\n",
    "    \n",
    "    print(f\"\\nSplit complete!\")\n",
    "    print(f\"  Train: {len(train_data)/total_lines*100:.1f}%\")\n",
    "    print(f\"  Val: {len(val_data)/total_lines*100:.1f}%\")\n",
    "    print(f\"  Test: {len(test_data)/total_lines*100:.1f}%\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Split your data\n",
    "    split_data(\n",
    "        input_file=\"cleaned-text.txt\",\n",
    "        train_file=\"train.txt\",\n",
    "        val_file=\"val.txt\",\n",
    "        test_file=\"test.txt\",\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.0,\n",
    "        test_ratio=0.2,\n",
    "        seed=42  # Change this for different splits\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25549996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel:\n",
    "    def __init__(self, sp_model_path):\n",
    "        # Load the SentencePiece model\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "        \n",
    "        # Get special token IDs\n",
    "        self.bos_id = self.sp.bos_id()  # Should be 1\n",
    "        self.eos_id = self.sp.eos_id()  # Should be 2\n",
    "        self.unk_id = self.sp.unk_id()  # Should be 0\n",
    "        self.pad_id = self.sp.pad_id()  # Should be 3\n",
    "        \n",
    "        # Count how many times we see each trigram (id1, id2, id3)\n",
    "        self.trigram_counts = defaultdict(int)\n",
    "        # Count how many times we see each bigram (id1, id2)\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        # Vocabulary size from SentencePiece\n",
    "        self.vocab_size = self.sp.vocab_size()\n",
    "        \n",
    "        print(f\"Loaded SentencePiece model: {sp_model_path}\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"BOS ID: {self.bos_id}, EOS ID: {self.eos_id}\")\n",
    "        \n",
    "    def train(self, text_file):\n",
    "        \"\"\"\n",
    "        Train the model on a text file.\n",
    "        The file should contain one sentence per line in Khmer.\n",
    "        \"\"\"\n",
    "        print(f\"\\nTraining on {text_file}...\")\n",
    "        sentence_count = 0\n",
    "        \n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize the sentence into token IDs\n",
    "                # add_bos=True adds one BOS token, add_eos=True adds EOS token\n",
    "                token_ids = self.sp.encode(line, add_bos=True, add_eos=True)\n",
    "                \n",
    "                # Add one more BOS at the beginning (so we have 2 total for trigrams)\n",
    "                token_ids = [self.bos_id] + token_ids\n",
    "                \n",
    "                # Count trigrams and bigrams\n",
    "                for i in range(len(token_ids) - 2):\n",
    "                    id1, id2, id3 = token_ids[i], token_ids[i+1], token_ids[i+2]\n",
    "                    \n",
    "                    # Count this specific trigram\n",
    "                    self.trigram_counts[(id1, id2, id3)] += 1\n",
    "                    \n",
    "                    # Count the bigram context\n",
    "                    self.bigram_counts[(id1, id2)] += 1\n",
    "                \n",
    "                sentence_count += 1\n",
    "                if sentence_count % 10000 == 0:\n",
    "                    print(f\"  Processed {sentence_count} sentences...\")\n",
    "        \n",
    "        print(f\"Training complete! Processed {sentence_count} sentences\")\n",
    "        print(f\"Unique trigrams: {len(self.trigram_counts)}\")\n",
    "        print(f\"Unique bigrams: {len(self.bigram_counts)}\")\n",
    "    \n",
    "    def get_probability(self, id1, id2, id3):\n",
    "        \"\"\"\n",
    "        Calculate P(id3 | id1, id2) with Laplacian smoothing.\n",
    "        \n",
    "        Formula: P(id3|id1,id2) = (count(id1,id2,id3) + 1) / (count(id1,id2) + V)\n",
    "        where V is the vocabulary size\n",
    "        \"\"\"\n",
    "        # Get counts (will be 0 if not seen)\n",
    "        trigram_count = self.trigram_counts[(id1, id2, id3)]\n",
    "        bigram_count = self.bigram_counts[(id1, id2)]\n",
    "        \n",
    "        # Apply Laplacian smoothing\n",
    "        probability = (trigram_count + 1) / (bigram_count + self.vocab_size)\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def calculate_perplexity(self, test_file):\n",
    "        \"\"\"\n",
    "        Calculate perplexity on a test file.\n",
    "        Lower perplexity = better model.\n",
    "        \"\"\"\n",
    "        print(f\"\\nCalculating perplexity on {test_file}...\")\n",
    "        total_log_prob = 0\n",
    "        total_tokens = 0\n",
    "        sentence_count = 0\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize the sentence\n",
    "                token_ids = self.sp.encode(line, add_bos=True, add_eos=True)\n",
    "                token_ids = [self.bos_id] + token_ids\n",
    "                \n",
    "                # Calculate probability for each token given its context\n",
    "                for i in range(2, len(token_ids)):\n",
    "                    id1, id2, id3 = token_ids[i-2], token_ids[i-1], token_ids[i]\n",
    "                    \n",
    "                    prob = self.get_probability(id1, id2, id3)\n",
    "                    \n",
    "                    # Add log probability\n",
    "                    total_log_prob += math.log(prob)\n",
    "                    total_tokens += 1\n",
    "                \n",
    "                sentence_count += 1\n",
    "                if sentence_count % 10000 == 0:\n",
    "                    print(f\"  Processed {sentence_count} sentences...\")\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        avg_log_prob = total_log_prob / total_tokens\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        print(f\"Perplexity calculation complete!\")\n",
    "        print(f\"Total tokens evaluated: {total_tokens}\")\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def generate_text(self, max_length=20, seed_text=None):\n",
    "        \"\"\"\n",
    "        Generate text using the trigram model (optional - for fun!).\n",
    "        \"\"\"\n",
    "        if seed_text:\n",
    "            token_ids = [self.bos_id] + self.sp.encode(seed_text, add_bos=False, add_eos=False)\n",
    "        else:\n",
    "            token_ids = [self.bos_id, self.bos_id]\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            if len(token_ids) < 2:\n",
    "                break\n",
    "                \n",
    "            # Get the last two tokens as context\n",
    "            id1, id2 = token_ids[-2], token_ids[-1]\n",
    "            \n",
    "            # Find the most probable next token\n",
    "            best_id = None\n",
    "            best_prob = 0\n",
    "            \n",
    "            for id3 in range(self.vocab_size):\n",
    "                prob = self.get_probability(id1, id2, id3)\n",
    "                if prob > best_prob:\n",
    "                    best_prob = prob\n",
    "                    best_id = id3\n",
    "            \n",
    "            # Stop if we generated EOS\n",
    "            if best_id == self.eos_id:\n",
    "                break\n",
    "                \n",
    "            token_ids.append(best_id)\n",
    "        \n",
    "        # Decode back to text\n",
    "        generated_text = self.sp.decode(token_ids[2:])  # Skip the two BOS tokens\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82c4de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to SentencePiece model\n",
    "sp_model_path = \"khmer_sp.model\"\n",
    "    \n",
    "# Path to training and test data\n",
    "train_file = \"train.txt\"\n",
    "test_file = \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d3cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SentencePiece model: khmer_sp.model\n",
      "Vocabulary size: 8000\n",
      "BOS ID: 1, EOS ID: 2\n",
      "\n",
      "Training on train.txt...\n",
      "  Processed 10000 sentences...\n",
      "  Processed 20000 sentences...\n",
      "  Processed 30000 sentences...\n",
      "  Processed 40000 sentences...\n",
      "  Processed 50000 sentences...\n",
      "  Processed 60000 sentences...\n",
      "  Processed 70000 sentences...\n",
      "  Processed 80000 sentences...\n",
      "  Processed 90000 sentences...\n",
      "  Processed 100000 sentences...\n",
      "  Processed 110000 sentences...\n",
      "  Processed 120000 sentences...\n",
      "  Processed 130000 sentences...\n",
      "  Processed 140000 sentences...\n",
      "  Processed 150000 sentences...\n",
      "  Processed 160000 sentences...\n",
      "Training complete! Processed 163701 sentences\n",
      "Unique trigrams: 3125694\n",
      "Unique bigrams: 1221720\n"
     ]
    }
   ],
   "source": [
    "# Create and train model\n",
    "model = TrigramModel(sp_model_path)\n",
    "model.train(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ce1152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating perplexity on test.txt...\n",
      "  Processed 10000 sentences...\n",
      "  Processed 20000 sentences...\n",
      "Perplexity calculation complete!\n",
      "Total tokens evaluated: 642369\n",
      "\n",
      "Final Perplexity: 3044.87\n",
      "(Lower perplexity = better model)\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "perplexity = model.calculate_perplexity(test_file)\n",
    "print(f\"\\nFinal Perplexity: {perplexity:.2f}\")\n",
    "print(\"(Lower perplexity = better model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0187a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating sample text ---\n",
      "Generated: ជាកិច្ចចាប់ផ្តើមឯកឧត្តមបានមកប្រមូលផ្តុំគ្នានៅទីលានធាន៤ផងកាន់អនិមិត្តវិមោក្ខផង\n"
     ]
    }
   ],
   "source": [
    "# Optional: Generate some text (just for fun!)\n",
    "print(\"\\n--- Generating sample text ---\")\n",
    "generated = model.generate_text(max_length=20)\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dcca8",
   "metadata": {},
   "source": [
    "### Testing with khmer word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c35bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef17d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramModel:\n",
    "    def __init__(self, use_unk=True):\n",
    "        # Count how many times we see each trigram (w1, w2, w3)\n",
    "        self.trigram_counts = defaultdict(int)\n",
    "        # Count how many times we see each bigram (w1, w2)\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        # Store all unique words we've seen\n",
    "        self.vocab = set()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.BOS = \"<BOS>\"  # Beginning of sentence\n",
    "        self.EOS = \"<EOS>\"  # End of sentence\n",
    "        self.UNK = \"<UNK>\"  # Unknown word (for unseen words in test)\n",
    "        self.use_unk = use_unk\n",
    "        \n",
    "    def train(self, sentences):\n",
    "        \"\"\"\n",
    "        Train the model on pre-tokenized sentences.\n",
    "        \n",
    "        Args:\n",
    "            sentences: List of lists, where each inner list contains Khmer words\n",
    "                      Example: [[\"សាលា\", \"រាជធានី\", \"ថា\"], [\"ខ្ញុំ\", \"ស្រលាញ់\"]]\n",
    "        \"\"\"\n",
    "        print(f\"Training on {len(sentences)} sentences...\")\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Add start and end tokens (we need 2 BOS for trigram context)\n",
    "            words = [self.BOS, self.BOS] + sentence + [self.EOS]\n",
    "            \n",
    "            # Add all words to vocabulary (including special tokens)\n",
    "            self.vocab.update(words)\n",
    "            \n",
    "            # Count trigrams and bigrams\n",
    "            for i in range(len(words) - 2):\n",
    "                w1, w2, w3 = words[i], words[i+1], words[i+2]\n",
    "                \n",
    "                # Count this specific trigram\n",
    "                self.trigram_counts[(w1, w2, w3)] += 1\n",
    "                \n",
    "                # Count the bigram context (first 2 words)\n",
    "                self.bigram_counts[(w1, w2)] += 1\n",
    "        \n",
    "        # Add UNK to vocabulary if enabled\n",
    "        if self.use_unk:\n",
    "            self.vocab.add(self.UNK)\n",
    "        \n",
    "        print(f\"Training complete!\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"  Unique trigrams: {len(self.trigram_counts)}\")\n",
    "        print(f\"  Unique bigrams: {len(self.bigram_counts)}\")\n",
    "    \n",
    "    def _handle_unk(self, word):\n",
    "        \"\"\"Replace unseen words with UNK token if enabled.\"\"\"\n",
    "        if self.use_unk and word not in self.vocab:\n",
    "            return self.UNK\n",
    "        return word\n",
    "    \n",
    "    def get_probability(self, w1, w2, w3):\n",
    "        \"\"\"\n",
    "        Calculate P(w3 | w1, w2) with Laplacian smoothing.\n",
    "        \n",
    "        Formula: P(w3|w1,w2) = (count(w1,w2,w3) + 1) / (count(w1,w2) + V)\n",
    "        where V is the vocabulary size\n",
    "        \"\"\"\n",
    "        # Handle unknown words\n",
    "        w1 = self._handle_unk(w1)\n",
    "        w2 = self._handle_unk(w2)\n",
    "        w3 = self._handle_unk(w3)\n",
    "        \n",
    "        vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Get counts (will be 0 if not seen)\n",
    "        trigram_count = self.trigram_counts[(w1, w2, w3)]\n",
    "        bigram_count = self.bigram_counts[(w1, w2)]\n",
    "        \n",
    "        # Apply Laplacian smoothing (add 1 to numerator, add V to denominator)\n",
    "        probability = (trigram_count + 1) / (bigram_count + vocab_size)\n",
    "        \n",
    "        return probability\n",
    "    \n",
    "    def calculate_perplexity(self, test_sentences):\n",
    "        \"\"\"\n",
    "        Calculate perplexity on test sentences.\n",
    "        Lower perplexity = better model.\n",
    "        \n",
    "        Args:\n",
    "            test_sentences: List of lists of pre-tokenized words\n",
    "        \n",
    "        Returns:\n",
    "            perplexity: Float value\n",
    "        \"\"\"\n",
    "        print(f\"\\nCalculating perplexity on {len(test_sentences)} test sentences...\")\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            # Add two BOS at start and EOS at end\n",
    "            words = [self.BOS, self.BOS] + sentence + [self.EOS]\n",
    "            \n",
    "            # Calculate probability for each word given its context\n",
    "            for i in range(2, len(words)):\n",
    "                w1 = words[i-2]  # Two words back\n",
    "                w2 = words[i-1]  # Previous word\n",
    "                w3 = words[i]     # Current word\n",
    "                \n",
    "                prob = self.get_probability(w1, w2, w3)\n",
    "                \n",
    "                # Add log probability\n",
    "                total_log_prob += math.log(prob)\n",
    "                total_words += 1\n",
    "        \n",
    "        # Calculate perplexity\n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        print(f\"Words evaluated: {total_words}\")\n",
    "        print(f\"Perplexity: {perplexity:.2f}\")\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to a file.\"\"\"\n",
    "        model_data = {\n",
    "            'trigram_counts': dict(self.trigram_counts),\n",
    "            'bigram_counts': dict(self.bigram_counts),\n",
    "            'vocab': list(self.vocab),\n",
    "            'BOS': self.BOS,\n",
    "            'EOS': self.EOS,\n",
    "            'UNK': self.UNK,\n",
    "            'use_unk': self.use_unk\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from a file.\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        # Convert string keys back to tuples for trigrams and bigrams\n",
    "        self.trigram_counts = defaultdict(int)\n",
    "        for key, value in model_data['trigram_counts'].items():\n",
    "            tuple_key = eval(key)\n",
    "            self.trigram_counts[tuple_key] = value\n",
    "        \n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        for key, value in model_data['bigram_counts'].items():\n",
    "            tuple_key = eval(key)\n",
    "            self.bigram_counts[tuple_key] = value\n",
    "        \n",
    "        self.vocab = set(model_data['vocab'])\n",
    "        self.BOS = model_data['BOS']\n",
    "        self.EOS = model_data['EOS']\n",
    "        self.UNK = model_data['UNK']\n",
    "        self.use_unk = model_data['use_unk']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocab)}\")\n",
    "\n",
    "\n",
    "# Helper function for loading data\n",
    "def load_tokenized_data(filepath):\n",
    "    \"\"\"\n",
    "    Load pre-tokenized data from a JSON file.\n",
    "    \n",
    "    Expected format: [[\"word1\", \"word2\"], [\"word3\", \"word4\"], ...]\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5eeab117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.json', 'r', encoding='utf-8') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "with open('test_set.json', 'r', encoding='utf-8') as f:\n",
    "    test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb83d834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 160442 sentences...\n",
      "Training complete!\n",
      "  Vocabulary size: 85031\n",
      "  Unique trigrams: 2627579\n",
      "  Unique bigrams: 999442\n"
     ]
    }
   ],
   "source": [
    "# Create and train model (with UNK handling)\n",
    "model = TrigramModel(use_unk=True)\n",
    "model.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5386742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example probabilities:\n",
      "P('ថា' | 'សាលា', 'រាជធានី') = 0.000024\n",
      "P('មិន' | 'រាជធានី', 'ថា') = 0.000024\n",
      "P('unseen_word' | 'ខ្ញុំ', 'ស្រលាញ់') = 0.000012\n"
     ]
    }
   ],
   "source": [
    "# Test some probabilities\n",
    "print(\"\\nExample probabilities:\")\n",
    "print(f\"P('ថា' | 'សាលា', 'រាជធានី') = {model.get_probability('សាលា', 'រាជធានី', 'ថា'):.6f}\")\n",
    "print(f\"P('មិន' | 'រាជធានី', 'ថា') = {model.get_probability('រាជធានី', 'ថា', 'មិន'):.6f}\")\n",
    "print(f\"P('unseen_word' | 'ខ្ញុំ', 'ស្រលាញ់') = {model.get_probability('ខ្ញុំ', 'ស្រលាញ់', 'unseen_word'):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "567e9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating perplexity on 40111 test sentences...\n",
      "Words evaluated: 1307684\n",
      "Perplexity: 19671.06\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on test data\n",
    "perplexity = model.calculate_perplexity(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create and train model (with UNK handling)\n",
    "    model = TrigramModel(use_unk=True)\n",
    "    model.train(train_data)\n",
    "    \n",
    "    # Test some probabilities\n",
    "    print(\"\\nExample probabilities:\")\n",
    "    print(f\"P('ថា' | 'សាលា', 'រាជធានី') = {model.get_probability('សាលា', 'រាជធានី', 'ថា'):.6f}\")\n",
    "    print(f\"P('មិន' | 'រាជធានី', 'ថា') = {model.get_probability('រាជធានី', 'ថា', 'មិន'):.6f}\")\n",
    "    print(f\"P('unseen_word' | 'ខ្ញុំ', 'ស្រលាញ់') = {model.get_probability('ខ្ញុំ', 'ស្រលាញ់', 'unseen_word'):.6f}\")\n",
    "    \n",
    "    # Calculate perplexity on test data\n",
    "    perplexity = model.calculate_perplexity(test_data)\n",
    "    \n",
    "    # If you have your data in JSON files:\n",
    "    # train_data = load_tokenized_data(\"train_tokenized.json\")\n",
    "    # test_data = load_tokenized_data(\"test_tokenized.json\")\n",
    "    # model.train(train_data)\n",
    "    # perplexity = model.calculate_perplexity(test_data)\n",
    "    \n",
    "    # Save/load model\n",
    "    # model.save_model(\"trigram_model.json\")\n",
    "    # model2 = TrigramModel()\n",
    "    # model2.load_model(\"trigram_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f63b8",
   "metadata": {},
   "source": [
    "### Interpolated N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f2637f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import pickle\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89d51b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNgramLM:\n",
    "    def __init__(self, sp_model_path, n=3, lambdas=None, smoothing='none', k=1.0):\n",
    "        \"\"\"\n",
    "        Initialize N-gram Language Model with Interpolation and optional Smoothing\n",
    "        \n",
    "        Args:\n",
    "            sp_model_path: Path to trained SentencePiece model (.model file)\n",
    "            n: Maximum n-gram order (default: 3 for trigrams)\n",
    "            lambdas: Interpolation weights for each n-gram order (should sum to 1.0)\n",
    "            smoothing: Smoothing method - 'none', 'laplace', or 'add-k' (default: 'none')\n",
    "            k: Smoothing parameter (default: 1.0 for Laplace, can use smaller values like 0.1)\n",
    "        \"\"\"\n",
    "        # Load SentencePiece model\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(sp_model_path)\n",
    "        \n",
    "        # Get special token IDs\n",
    "        self.bos_id = self.sp.bos_id()\n",
    "        self.eos_id = self.sp.eos_id()\n",
    "        self.unk_id = self.sp.unk_id()\n",
    "        self.pad_id = self.sp.pad_id()\n",
    "        \n",
    "        self.n = n\n",
    "        # Lambdas should sum to 1.0 for proper interpolation\n",
    "        self.lambdas = lambdas or [0.1, 0.3, 0.6]  # unigram, bigram, trigram\n",
    "        assert abs(sum(self.lambdas) - 1.0) < 1e-6, \"Lambdas should sum to 1.0\"\n",
    "        \n",
    "        self.smoothing = smoothing\n",
    "        self.k = k\n",
    "        \n",
    "        # N-gram storage: ngrams[k-1] stores k-grams\n",
    "        self.ngrams = [defaultdict(Counter) for _ in range(n)]\n",
    "        self.token_freq = Counter()  # Frequency of each token ID\n",
    "        self.total_tokens = 0\n",
    "        self.vocab_size = self.sp.vocab_size()\n",
    "        \n",
    "        print(f\"Loaded SentencePiece model: {sp_model_path}\")\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    def train(self, text_file):\n",
    "        \"\"\"\n",
    "        Train the model on a text file.\n",
    "        The file should contain one sentence per line.\n",
    "        \n",
    "        Args:\n",
    "            text_file: Path to text file with one sentence per line\n",
    "        \"\"\"\n",
    "        print(f\"\\nTraining on {text_file}...\")\n",
    "        sentence_count = 0\n",
    "        \n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize the sentence into token IDs\n",
    "                token_ids = self.sp.encode(line, add_bos=True, add_eos=True)\n",
    "                \n",
    "                # Update token frequencies\n",
    "                self.token_freq.update(token_ids)\n",
    "                self.total_tokens += len(token_ids)\n",
    "                \n",
    "                # Build n-grams of all orders\n",
    "                for i in range(len(token_ids)):\n",
    "                    for order in range(1, self.n + 1):\n",
    "                        if i - order + 1 >= 0:\n",
    "                            # Context: previous (order-1) tokens\n",
    "                            context = tuple(token_ids[i - order + 1:i])\n",
    "                            # Current token\n",
    "                            token = token_ids[i]\n",
    "                            # Store in ngrams[order-1]\n",
    "                            self.ngrams[order - 1][context][token] += 1\n",
    "                \n",
    "                sentence_count += 1\n",
    "                if sentence_count % 10000 == 0:\n",
    "                    print(f\"  Processed {sentence_count} sentences...\")\n",
    "        \n",
    "        print(f\"Training complete! Processed {sentence_count} sentences\")\n",
    "        print(f\"Total tokens: {self.total_tokens}\")\n",
    "        for order in range(1, self.n + 1):\n",
    "            print(f\"  Unique {order}-grams: {len(self.ngrams[order - 1])}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Probability Calculation\n",
    "    # -----------------------------\n",
    "    def prob(self, token_id, context):\n",
    "        \"\"\"\n",
    "        Calculate interpolated probability P(token_id|context)\n",
    "        Uses linear interpolation across all n-gram orders\n",
    "        Supports Laplace/Add-k smoothing\n",
    "        \n",
    "        Args:\n",
    "            token_id: Token ID to calculate probability for\n",
    "            context: Tuple of previous token IDs\n",
    "        \n",
    "        Returns:\n",
    "            Probability value\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        prob = 0.0\n",
    "\n",
    "        for order in range(1, self.n + 1):\n",
    "            lambda_k = self.lambdas[order - 1]\n",
    "            \n",
    "            # Get the appropriate context length for this n-gram order\n",
    "            if order == 1:\n",
    "                # Unigram with smoothing\n",
    "                p_k = self._unigram_prob(token_id)\n",
    "            else:\n",
    "                # Higher order n-grams with smoothing\n",
    "                ctx_len = min(order - 1, len(context))\n",
    "                ctx = context[-ctx_len:] if ctx_len > 0 else ()\n",
    "                p_k = self._ngram_prob(token_id, ctx, order)\n",
    "            \n",
    "            prob += lambda_k * p_k\n",
    "\n",
    "        return prob\n",
    "    \n",
    "    def _unigram_prob(self, token_id):\n",
    "        \"\"\"Calculate unigram probability with optional smoothing\"\"\"\n",
    "        if self.smoothing in ['laplace', 'add-k']:\n",
    "            # P(token) = (count(token) + k) / (total_tokens + k * vocab_size)\n",
    "            numerator = self.token_freq.get(token_id, 0) + self.k\n",
    "            denominator = self.total_tokens + self.k * self.vocab_size\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            # No smoothing\n",
    "            return self.token_freq.get(token_id, 0) / max(self.total_tokens, 1)\n",
    "    \n",
    "    def _ngram_prob(self, token_id, context, order):\n",
    "        \"\"\"Calculate n-gram probability with optional smoothing\"\"\"\n",
    "        counter = self.ngrams[order - 1].get(context)\n",
    "        \n",
    "        if self.smoothing in ['laplace', 'add-k']:\n",
    "            # P(token|context) = (count(context, token) + k) / (count(context) + k * vocab_size)\n",
    "            token_count = counter[token_id] if counter else 0\n",
    "            context_count = sum(counter.values()) if counter else 0\n",
    "            \n",
    "            numerator = token_count + self.k\n",
    "            denominator = context_count + self.k * self.vocab_size\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            # No smoothing\n",
    "            if counter and token_id in counter:\n",
    "                return counter[token_id] / sum(counter.values())\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Text Prediction\n",
    "    # -----------------------------\n",
    "    def predict_next_token(self, text, top_k=5):\n",
    "        \"\"\"\n",
    "        Predict next token given input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "            top_k: Number of predictions to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (token_string, probability) tuples\n",
    "        \"\"\"\n",
    "        # Encode the input text\n",
    "        token_ids = self.sp.encode(text, add_bos=False, add_eos=False)\n",
    "        \n",
    "        # Get context (last n-1 tokens)\n",
    "        context = tuple(token_ids[-(self.n - 1):]) if len(token_ids) >= self.n - 1 else tuple(token_ids)\n",
    "        \n",
    "        # Calculate probability for all tokens in vocabulary\n",
    "        token_probs = []\n",
    "        for token_id in range(self.vocab_size):\n",
    "            prob = self.prob(token_id, context)\n",
    "            if prob > 0:  # Only consider tokens with non-zero probability\n",
    "                token_probs.append((token_id, prob))\n",
    "        \n",
    "        # Sort by probability and get top_k\n",
    "        token_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_predictions = token_probs[:top_k]\n",
    "        \n",
    "        # Decode token IDs to strings\n",
    "        results = []\n",
    "        for token_id, prob in top_predictions:\n",
    "            token_string = self.sp.id_to_piece(token_id)\n",
    "            results.append((token_string, prob))\n",
    "        \n",
    "        return results\n",
    "\n",
    "    # -----------------------------\n",
    "    # Perplexity Evaluation\n",
    "    # -----------------------------\n",
    "    def perplexity(self, test_file):\n",
    "        \"\"\"\n",
    "        Calculate perplexity on test data\n",
    "        Lower perplexity = better model\n",
    "        \n",
    "        Args:\n",
    "            test_file: Path to test text file (one sentence per line)\n",
    "        \n",
    "        Returns:\n",
    "            Perplexity value\n",
    "        \"\"\"\n",
    "        print(f\"\\nCalculating perplexity on {test_file}...\")\n",
    "        log_prob = 0.0\n",
    "        count = 0\n",
    "        sentence_count = 0\n",
    "\n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # Tokenize the sentence\n",
    "                token_ids = self.sp.encode(line, add_bos=True, add_eos=True)\n",
    "                \n",
    "                # Calculate probability for each token\n",
    "                for i in range(len(token_ids)):\n",
    "                    # Get context (previous n-1 tokens)\n",
    "                    context = tuple(token_ids[max(0, i - self.n + 1):i])\n",
    "                    \n",
    "                    # Get probability of current token\n",
    "                    p = self.prob(token_ids[i], context)\n",
    "                    \n",
    "                    # Add log probability (with smoothing to avoid log(0))\n",
    "                    log_prob += math.log(p + 1e-10)\n",
    "                    count += 1\n",
    "                \n",
    "                sentence_count += 1\n",
    "                if sentence_count % 10000 == 0:\n",
    "                    print(f\"  Processed {sentence_count} sentences...\")\n",
    "\n",
    "        perplexity_value = math.exp(-log_prob / count)\n",
    "        print(f\"Perplexity calculation complete!\")\n",
    "        print(f\"Total tokens evaluated: {count}\")\n",
    "        print(f\"Perplexity: {perplexity_value:.2f}\")\n",
    "        \n",
    "        return perplexity_value\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save and Load Model\n",
    "    # -----------------------------\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: path where to save the model (e.g., 'khmer_lm.pkl')\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'sp_model_path': self.sp.model_file(),\n",
    "            'n': self.n,\n",
    "            'lambdas': self.lambdas,\n",
    "            'smoothing': self.smoothing,\n",
    "            'k': self.k,\n",
    "            'ngrams': self.ngrams,\n",
    "            'token_freq': self.token_freq,\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: path to the saved model file\n",
    "        \n",
    "        Returns:\n",
    "            InterpolatedNgramLM: loaded model instance\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create new instance\n",
    "        model = cls(\n",
    "            sp_model_path=model_data['sp_model_path'],\n",
    "            n=model_data['n'], \n",
    "            lambdas=model_data['lambdas'],\n",
    "            smoothing=model_data.get('smoothing', 'none'),\n",
    "            k=model_data.get('k', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Restore all attributes\n",
    "        model.ngrams = model_data['ngrams']\n",
    "        model.token_freq = model_data['token_freq']\n",
    "        model.total_tokens = model_data['total_tokens']\n",
    "        model.vocab_size = model_data.get('vocab_size', model.sp.vocab_size())\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67063b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SentencePiece model: khmer_sp.model\n",
      "Vocabulary size: 8000\n"
     ]
    }
   ],
   "source": [
    "model = InterpolatedNgramLM(\n",
    "        sp_model_path=\"khmer_sp.model\",\n",
    "        n=3,  # Trigram model\n",
    "        lambdas=[0.1, 0.3, 0.6],  # Weights for unigram, bigram, trigram\n",
    "        smoothing='laplace',  # Use Laplace smoothing\n",
    "        k=1.0  # Smoothing parameter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98618353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on train.txt...\n",
      "  Processed 10000 sentences...\n",
      "  Processed 20000 sentences...\n",
      "  Processed 30000 sentences...\n",
      "  Processed 40000 sentences...\n",
      "  Processed 50000 sentences...\n",
      "  Processed 60000 sentences...\n",
      "  Processed 70000 sentences...\n",
      "  Processed 80000 sentences...\n",
      "  Processed 90000 sentences...\n",
      "  Processed 100000 sentences...\n",
      "  Processed 110000 sentences...\n",
      "  Processed 120000 sentences...\n",
      "  Processed 130000 sentences...\n",
      "  Processed 140000 sentences...\n",
      "  Processed 150000 sentences...\n",
      "  Processed 160000 sentences...\n",
      "Training complete! Processed 163701 sentences\n",
      "Total tokens: 5282449\n",
      "  Unique 1-grams: 1\n",
      "  Unique 2-grams: 7997\n",
      "  Unique 3-grams: 1221719\n",
      "\n",
      "Calculating perplexity on test.txt...\n",
      "  Processed 10000 sentences...\n",
      "  Processed 20000 sentences...\n",
      "  Processed 30000 sentences...\n",
      "  Processed 40000 sentences...\n",
      "Perplexity calculation complete!\n",
      "Total tokens evaluated: 1325577\n",
      "Perplexity: 1296.73\n"
     ]
    }
   ],
   "source": [
    "# Train on your data\n",
    "model.train(\"train.txt\")\n",
    "\n",
    "# Calculate perplexity on test set\n",
    "perplexity = model.perplexity(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23812b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize model with SentencePiece\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Predict next token\n",
    "    predictions = model.predict_next_token(\"ខ្ញុំ ស្រលាញ់\", top_k=5)\n",
    "    print(\"\\nTop 5 predictions:\")\n",
    "    for token, prob in predictions:\n",
    "        print(f\"  {token}: {prob:.6f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"khmer_interpolated_lm.pkl\")\n",
    "    \n",
    "    # Load the model later\n",
    "    # loaded_model = InterpolatedNgramLM.load(\"khmer_interpolated_lm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ac550",
   "metadata": {},
   "source": [
    "## Testing with word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2b400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpolatedNgramLM:\n",
    "    def __init__(self, n=3, lambdas=None, smoothing='none', k=1.0):\n",
    "        \"\"\"\n",
    "        Initialize N-gram Language Model with Interpolation and optional Smoothing\n",
    "        \n",
    "        Args:\n",
    "            n: Maximum n-gram order (default: 3 for trigrams)\n",
    "            lambdas: Interpolation weights for each n-gram order (should sum to 1.0)\n",
    "            smoothing: Smoothing method - 'none', 'laplace', or 'add-k' (default: 'none')\n",
    "            k: Smoothing parameter (default: 1.0 for Laplace, can use smaller values like 0.1)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        # Lambdas should sum to 1.0 for proper interpolation\n",
    "        self.lambdas = lambdas or [0.1, 0.3, 0.6]  # unigram, bigram, trigram\n",
    "        assert abs(sum(self.lambdas) - 1.0) < 1e-6, \"Lambdas should sum to 1.0\"\n",
    "        \n",
    "        self.smoothing = smoothing\n",
    "        self.k = k\n",
    "        \n",
    "        self.ngrams = [defaultdict(Counter) for _ in range(n)]\n",
    "        self.word_freq = Counter()\n",
    "        self.vocabulary = set()\n",
    "        self.prefix_set = set()\n",
    "        self.total_tokens = 0\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # Training\n",
    "    # -----------------------------\n",
    "    def train(self, tokenized_sentences):\n",
    "        for sent in tokenized_sentences:\n",
    "            self.word_freq.update(sent)\n",
    "            self.vocabulary.update(sent)\n",
    "            self.total_tokens += len(sent)\n",
    "\n",
    "            for i in range(len(sent)):\n",
    "                # Build n-grams of all orders\n",
    "                for k in range(1, self.n + 1):\n",
    "                    if i - k + 1 >= 0:\n",
    "                        context = tuple(sent[i - k + 1:i])\n",
    "                        word = sent[i]\n",
    "                        self.ngrams[k - 1][context][word] += 1\n",
    "\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self._build_prefix_set()\n",
    "\n",
    "    def _build_prefix_set(self):\n",
    "        \"\"\"Build set of all possible prefixes from vocabulary\"\"\"\n",
    "        for word in self.vocabulary:\n",
    "            # Add all prefixes of each word (except the full word itself)\n",
    "            for i in range(1, len(word)):\n",
    "                self.prefix_set.add(word[:i])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Probability Calculation\n",
    "    # -----------------------------\n",
    "    def prob(self, word, context):\n",
    "        \"\"\"\n",
    "        Calculate interpolated probability P(word|context)\n",
    "        Uses linear interpolation across all n-gram orders\n",
    "        Supports Laplace/Add-k smoothing\n",
    "        \"\"\"\n",
    "        context = tuple(context)\n",
    "        prob = 0.0\n",
    "\n",
    "        for k in range(1, self.n + 1):\n",
    "            lambda_k = self.lambdas[k - 1]\n",
    "            \n",
    "            # Get the appropriate context length for this n-gram order\n",
    "            if k == 1:\n",
    "                # Unigram with smoothing\n",
    "                ctx = ()\n",
    "                p_k = self._unigram_prob(word)\n",
    "            else:\n",
    "                # Higher order n-grams with smoothing\n",
    "                ctx_len = min(k - 1, len(context))\n",
    "                ctx = context[-ctx_len:] if ctx_len > 0 else ()\n",
    "                p_k = self._ngram_prob(word, ctx, k)\n",
    "            \n",
    "            prob += lambda_k * p_k\n",
    "\n",
    "        return prob\n",
    "    \n",
    "    def _unigram_prob(self, word):\n",
    "        \"\"\"Calculate unigram probability with optional smoothing\"\"\"\n",
    "        if self.smoothing in ['laplace', 'add-k']:\n",
    "            # P(w) = (count(w) + k) / (total_tokens + k * vocab_size)\n",
    "            numerator = self.word_freq.get(word, 0) + self.k\n",
    "            denominator = self.total_tokens + self.k * self.vocab_size\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            # No smoothing\n",
    "            return self.word_freq.get(word, 0) / max(self.total_tokens, 1)\n",
    "    \n",
    "    def _ngram_prob(self, word, context, k):\n",
    "        \"\"\"Calculate n-gram probability with optional smoothing\"\"\"\n",
    "        counter = self.ngrams[k - 1].get(context)\n",
    "        \n",
    "        if self.smoothing in ['laplace', 'add-k']:\n",
    "            # P(w|context) = (count(context, w) + k) / (count(context) + k * vocab_size)\n",
    "            word_count = counter[word] if counter else 0\n",
    "            context_count = sum(counter.values()) if counter else 0\n",
    "            \n",
    "            numerator = word_count + self.k\n",
    "            denominator = context_count + self.k * self.vocab_size\n",
    "            return numerator / denominator\n",
    "        else:\n",
    "            # No smoothing\n",
    "            if counter and word in counter:\n",
    "                return counter[word] / sum(counter.values())\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "    def get_distribution(self, context, vocab_subset=None):\n",
    "        \"\"\"\n",
    "        Get probability distribution over vocabulary (or a subset)\n",
    "        \"\"\"\n",
    "        if vocab_subset is None:\n",
    "            vocab_subset = self.vocabulary\n",
    "        \n",
    "        return {\n",
    "            word: self.prob(word, context)\n",
    "            for word in vocab_subset\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prefix-aware prediction\n",
    "    # -----------------------------\n",
    "    def predict(self, tokens, top_k=5):\n",
    "        \"\"\"\n",
    "        Predict next word given context tokens.\n",
    "        If last token is a prefix, restrict predictions to words starting with that prefix.\n",
    "        \n",
    "        Args:\n",
    "            tokens: list of words/tokens representing the context\n",
    "            top_k: number of predictions to return\n",
    "        \n",
    "        Returns:\n",
    "            list of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        prefix = \"\"\n",
    "        context = tuple(tokens)\n",
    "\n",
    "        # Check if last token is an incomplete word (prefix)\n",
    "        if tokens:\n",
    "            last = tokens[-1]\n",
    "            # It's a prefix if it's in prefix_set OR not in vocabulary\n",
    "            if last in self.prefix_set or (last not in self.vocabulary and len(last) > 0):\n",
    "                prefix = last\n",
    "                context = tuple(tokens[:-1])\n",
    "\n",
    "        # Get probability distribution over all words\n",
    "        dist = self.get_distribution(context)\n",
    "\n",
    "        # Filter by prefix if applicable\n",
    "        if prefix:\n",
    "            candidates = {\n",
    "                w: p for w, p in dist.items()\n",
    "                if w.startswith(prefix)\n",
    "            }\n",
    "        else:\n",
    "            candidates = dist\n",
    "\n",
    "        # Renormalize probabilities after filtering\n",
    "        Z = sum(candidates.values())\n",
    "        if Z > 0:\n",
    "            candidates = {w: p / Z for w, p in candidates.items()}\n",
    "        else:\n",
    "            # Fallback: if no candidates, return empty or most common words\n",
    "            return []\n",
    "\n",
    "        # Sort by probability and return top_k with probabilities\n",
    "        sorted_predictions = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        return sorted_predictions\n",
    "\n",
    "    def predict_words_only(self, tokens, top_k=5):\n",
    "        \"\"\"Convenience method that returns only words without probabilities\"\"\"\n",
    "        predictions = self.predict(tokens, top_k)\n",
    "        return [word for word, prob in predictions]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Perplexity Evaluation\n",
    "    # -----------------------------\n",
    "    def perplexity(self, tokenized_sentences):\n",
    "        \"\"\"\n",
    "        Calculate perplexity on test data\n",
    "        Lower perplexity = better model\n",
    "        \"\"\"\n",
    "        log_prob = 0.0\n",
    "        count = 0\n",
    "\n",
    "        for sent in tokenized_sentences:\n",
    "            for i in range(len(sent)):\n",
    "                # Get context (previous n-1 words)\n",
    "                context = tuple(sent[max(0, i - self.n + 1):i])\n",
    "                \n",
    "                # Get probability of current word\n",
    "                p = self.prob(sent[i], context)\n",
    "                \n",
    "                # Add log probability (with smoothing to avoid log(0))\n",
    "                log_prob += math.log(p + 1e-10)\n",
    "                count += 1\n",
    "\n",
    "        return math.exp(-log_prob / count)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save and Load Model\n",
    "    # -----------------------------\n",
    "    def save(self, filepath):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: path where to save the model (e.g., 'khmer_lm.pkl')\n",
    "        \"\"\"\n",
    "        model_data = {\n",
    "            'n': self.n,\n",
    "            'lambdas': self.lambdas,\n",
    "            'smoothing': self.smoothing,\n",
    "            'k': self.k,\n",
    "            'ngrams': self.ngrams,\n",
    "            'word_freq': self.word_freq,\n",
    "            'vocabulary': self.vocabulary,\n",
    "            'prefix_set': self.prefix_set,\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file\n",
    "        \n",
    "        Args:\n",
    "            filepath: path to the saved model file\n",
    "        \n",
    "        Returns:\n",
    "            InterpolatedNgramLM: loaded model instance\n",
    "        \"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        # Create new instance\n",
    "        model = cls(\n",
    "            n=model_data['n'], \n",
    "            lambdas=model_data['lambdas'],\n",
    "            smoothing=model_data.get('smoothing', 'none'),\n",
    "            k=model_data.get('k', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Restore all attributes\n",
    "        model.ngrams = model_data['ngrams']\n",
    "        model.word_freq = model_data['word_freq']\n",
    "        model.vocabulary = model_data['vocabulary']\n",
    "        model.prefix_set = model_data['prefix_set']\n",
    "        model.total_tokens = model_data['total_tokens']\n",
    "        model.vocab_size = model_data.get('vocab_size', len(model_data['vocabulary']))\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8657d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_k_accuracy(model, tokenized_sentences, k=5):\n",
    "    \"\"\"\n",
    "    Calculate top-k accuracy on test data.\n",
    "    \n",
    "    Top-k accuracy measures how often the actual next word appears \n",
    "    in the model's top-k predictions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained InterpolatedNgramLM instance\n",
    "        tokenized_sentences: List of tokenized sentences to evaluate on\n",
    "        k: Number of top predictions to consider (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        float: Top-k accuracy (between 0 and 1)\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sent in tokenized_sentences:\n",
    "        for i in range(len(sent)):\n",
    "            # Get context (previous words)\n",
    "            context = sent[max(0, i - model.n + 1):i]\n",
    "            \n",
    "            # Get actual next word\n",
    "            actual_word = sent[i]\n",
    "            \n",
    "            # Get top-k predictions (returns list of words only)\n",
    "            predictions = model.predict_words_only(context, top_k=k)\n",
    "            \n",
    "            # Check if actual word is in top-k predictions\n",
    "            if actual_word in predictions:\n",
    "                correct += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3004ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577a57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.json', 'r', encoding='utf-8') as f:\n",
    "    train_set = json.load(f)\n",
    "\n",
    "with open('test_set.json', 'r', encoding='utf-8') as f:\n",
    "    test_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81627c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_test = test_set[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8103fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = InterpolatedNgramLM(n=3, lambdas=[0.6, 0.35, 0.05], smoothing=\"laplace\")\n",
    "model.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30ca182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 0.0059\n",
      "Top-5 Accuracy: 0.0236\n",
      "Top-10 Accuracy: 0.0361\n"
     ]
    }
   ],
   "source": [
    "accuracy_top1 = calculate_top_k_accuracy(model, topk_test, k=1)\n",
    "accuracy_top5 = calculate_top_k_accuracy(model, topk_test, k=5)\n",
    "accuracy_top10 = calculate_top_k_accuracy(model, topk_test, k=10)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {accuracy_top1:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {accuracy_top5:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {accuracy_top10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e14da8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1495.0354525241103\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.8, 0.15, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d07ac767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1450.129298427265\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.7, 0.25, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac65f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1444.8114943683643\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.6, 0.35, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc268b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1495.3259092829674\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.45, 0.5, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8590033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1573.1683602674264\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.35, 0.6, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4fed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1852.5274411902947\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.25, 0.7, 0.05])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4b22d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1852.5274411902947\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.1, 0.7, 0.2])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dee51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 2485.2679134048326\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.1, 0.6, 0.3])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc2c65",
   "metadata": {},
   "source": [
    "## Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd19dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = InterpolatedNgramLM(n=2, lambdas=[0.8, 0.2], smoothing=\"laplace\")\n",
    "model.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0128d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1758.8460583745057\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.2, 0.8])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9977fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1487.708893326101\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.4, 0.6])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a4b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 3590.6880641120074\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.0, 1.0])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e854bf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1396.5005677310155\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.7, 0.3])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff48ff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1512.084487718711\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.9, 0.1])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e327e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity: 1427.3634439645875\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity on training data (lambdas=[0.8, 0.2])\n",
    "ppl = model.perplexity(test_set)\n",
    "print(f\"\\nPerplexity: {ppl}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
